# frozen_string_literal: true

require_relative "assistant_transcriber"
require_relative "assistant_model"
require_relative "assistant_voice"
require_relative "assistant_first_message_mode"
require_relative "assistant_voicemail_detection"
require_relative "assistant_client_messages_item"
require_relative "assistant_server_messages_item"
require_relative "assistant_background_sound"
require_relative "transport_configuration_twilio"
require_relative "langfuse_observability_plan"
require_relative "assistant_credentials_item"
require_relative "assistant_hooks_item"
require_relative "compliance_plan"
require_relative "background_speech_denoising_plan"
require_relative "analysis_plan"
require_relative "artifact_plan"
require_relative "start_speaking_plan"
require_relative "stop_speaking_plan"
require_relative "monitor_plan"
require_relative "server"
require_relative "keypad_input_plan"
require "date"
require "ostruct"
require "json"

module Vapi
  class Assistant
    # @return [Vapi::AssistantTranscriber] These are the options for the assistant's transcriber.
    attr_reader :transcriber
    # @return [Vapi::AssistantModel] These are the options for the assistant's LLM.
    attr_reader :model
    # @return [Vapi::AssistantVoice] These are the options for the assistant's voice.
    attr_reader :voice
    # @return [String] This is the first message that the assistant will say. This can also be a URL to
    #  a containerized audio file (mp3, wav, etc.).
    #  If unspecified, assistant will wait for user to speak and use the model to
    #  respond once they speak.
    attr_reader :first_message
    # @return [Boolean]
    attr_reader :first_message_interruptions_enabled
    # @return [Vapi::AssistantFirstMessageMode] This is the mode for the first message. Default is 'assistant-speaks-first'.
    #  Use:
    #  - 'assistant-speaks-first' to have the assistant speak first.
    #  - 'assistant-waits-for-user' to have the assistant wait for the user to speak
    #  first.
    #  - 'assistant-speaks-first-with-model-generated-message' to have the assistant
    #  speak first with a message generated by the model based on the conversation
    #  state. (`assistant.model.messages` at call start, `call.messages` at squad
    #  transfer points).
    #  @default 'assistant-speaks-first'
    attr_reader :first_message_mode
    # @return [Vapi::AssistantVoicemailDetection] These are the settings to configure or disable voicemail detection.
    #  Alternatively, voicemail detection can be configured using the
    #  model.tools=[VoicemailTool].
    #  By default, voicemail detection is disabled.
    attr_reader :voicemail_detection
    # @return [Array<Vapi::AssistantClientMessagesItem>] These are the messages that will be sent to your Client SDKs. Default is
    #  tool-calls,user-interrupted,voice-input,workflow.node.started,assistant.started.
    #  You can check the shape of the messages in ClientMessage schema.
    attr_reader :client_messages
    # @return [Array<Vapi::AssistantServerMessagesItem>] These are the messages that will be sent to your Server URL. Default is
    #  tination-request,handoff-destination-request,user-interrupted,assistant.started.
    #  You can check the shape of the messages in ServerMessage schema.
    attr_reader :server_messages
    # @return [Float] This is the maximum number of seconds that the call will last. When the call
    #  reaches this duration, it will be ended.
    #  @default 600 (10 minutes)
    attr_reader :max_duration_seconds
    # @return [Vapi::AssistantBackgroundSound] This is the background sound in the call. Default for phone calls is 'office'
    #  and default for web calls is 'off'.
    #  You can also provide a custom sound by providing a URL to an audio file.
    attr_reader :background_sound
    # @return [Boolean] This determines whether the model's output is used in conversation history
    #  rather than the transcription of assistant's speech.
    #  Default `false` while in beta.
    #  @default false
    attr_reader :model_output_in_messages_enabled
    # @return [Array<Vapi::TransportConfigurationTwilio>] These are the configurations to be passed to the transport providers of
    #  assistant's calls, like Twilio. You can store multiple configurations for
    #  different transport providers. For a call, only the configuration matching the
    #  call transport provider is used.
    attr_reader :transport_configurations
    # @return [Vapi::LangfuseObservabilityPlan] This is the plan for observability of assistant's calls.
    #  Currently, only Langfuse is supported.
    attr_reader :observability_plan
    # @return [Array<Vapi::AssistantCredentialsItem>] These are dynamic credentials that will be used for the assistant calls. By
    #  default, all the credentials are available for use in the call but you can
    #  supplement an additional credentials using this. Dynamic credentials override
    #  existing credentials.
    attr_reader :credentials
    # @return [Array<Vapi::AssistantHooksItem>] This is a set of actions that will be performed on certain events.
    attr_reader :hooks
    # @return [String] This is the name of the assistant.
    #  This is required when you want to transfer between assistants in a call.
    attr_reader :name
    # @return [String] This is the message that the assistant will say if the call is forwarded to
    #  voicemail.
    #  If unspecified, it will hang up.
    attr_reader :voicemail_message
    # @return [String] This is the message that the assistant will say if it ends the call.
    #  If unspecified, it will hang up without saying anything.
    attr_reader :end_call_message
    # @return [Array<String>] This list contains phrases that, if spoken by the assistant, will trigger the
    #  call to be hung up. Case insensitive.
    attr_reader :end_call_phrases
    # @return [Vapi::CompliancePlan]
    attr_reader :compliance_plan
    # @return [Hash{String => Object}] This is for metadata you want to store on the assistant.
    attr_reader :metadata
    # @return [Vapi::BackgroundSpeechDenoisingPlan] This enables filtering of noise and background speech while the user is talking.
    #  Features:
    #  - Smart denoising using Krisp
    #  - Fourier denoising
    #  Smart denoising can be combined with or used independently of Fourier denoising.
    #  Order of precedence:
    #  - Smart denoising
    #  - Fourier denoising
    attr_reader :background_speech_denoising_plan
    # @return [Vapi::AnalysisPlan] This is the plan for analysis of assistant's calls. Stored in `call.analysis`.
    attr_reader :analysis_plan
    # @return [Vapi::ArtifactPlan] This is the plan for artifacts generated during assistant's calls. Stored in
    #  `call.artifact`.
    attr_reader :artifact_plan
    # @return [Vapi::StartSpeakingPlan] This is the plan for when the assistant should start talking.
    #  You should configure this if you're running into these issues:
    #  - The assistant is too slow to start talking after the customer is done
    #  speaking.
    #  - The assistant is too fast to start talking after the customer is done
    #  speaking.
    #  - The assistant is so fast that it's actually interrupting the customer.
    attr_reader :start_speaking_plan
    # @return [Vapi::StopSpeakingPlan] This is the plan for when assistant should stop talking on customer
    #  interruption.
    #  You should configure this if you're running into these issues:
    #  - The assistant is too slow to recognize customer's interruption.
    #  - The assistant is too fast to recognize customer's interruption.
    #  - The assistant is getting interrupted by phrases that are just acknowledgments.
    #  - The assistant is getting interrupted by background noises.
    #  - The assistant is not properly stopping -- it starts talking right after
    #  getting interrupted.
    attr_reader :stop_speaking_plan
    # @return [Vapi::MonitorPlan] This is the plan for real-time monitoring of the assistant's calls.
    #  Usage:
    #  - To enable live listening of the assistant's calls, set
    #  `monitorPlan.listenEnabled` to `true`.
    #  - To enable live control of the assistant's calls, set
    #  `monitorPlan.controlEnabled` to `true`.
    attr_reader :monitor_plan
    # @return [Array<String>] These are the credentials that will be used for the assistant calls. By default,
    #  all the credentials are available for use in the call but you can provide a
    #  subset using this.
    attr_reader :credential_ids
    # @return [Vapi::Server] This is where Vapi will send webhooks. You can find all webhooks available along
    #  with their shape in ServerMessage schema.
    #  The order of precedence is:
    #  1. assistant.server.url
    #  2. phoneNumber.serverUrl
    #  3. org.serverUrl
    attr_reader :server
    # @return [Vapi::KeypadInputPlan]
    attr_reader :keypad_input_plan
    # @return [String] This is the unique identifier for the assistant.
    attr_reader :id
    # @return [String] This is the unique identifier for the org that this assistant belongs to.
    attr_reader :org_id
    # @return [DateTime] This is the ISO 8601 date-time string of when the assistant was created.
    attr_reader :created_at
    # @return [DateTime] This is the ISO 8601 date-time string of when the assistant was last updated.
    attr_reader :updated_at
    # @return [OpenStruct] Additional properties unmapped to the current class definition
    attr_reader :additional_properties
    # @return [Object]
    attr_reader :_field_set
    protected :_field_set

    OMIT = Object.new

    # @param transcriber [Vapi::AssistantTranscriber] These are the options for the assistant's transcriber.
    # @param model [Vapi::AssistantModel] These are the options for the assistant's LLM.
    # @param voice [Vapi::AssistantVoice] These are the options for the assistant's voice.
    # @param first_message [String] This is the first message that the assistant will say. This can also be a URL to
    #  a containerized audio file (mp3, wav, etc.).
    #  If unspecified, assistant will wait for user to speak and use the model to
    #  respond once they speak.
    # @param first_message_interruptions_enabled [Boolean]
    # @param first_message_mode [Vapi::AssistantFirstMessageMode] This is the mode for the first message. Default is 'assistant-speaks-first'.
    #  Use:
    #  - 'assistant-speaks-first' to have the assistant speak first.
    #  - 'assistant-waits-for-user' to have the assistant wait for the user to speak
    #  first.
    #  - 'assistant-speaks-first-with-model-generated-message' to have the assistant
    #  speak first with a message generated by the model based on the conversation
    #  state. (`assistant.model.messages` at call start, `call.messages` at squad
    #  transfer points).
    #  @default 'assistant-speaks-first'
    # @param voicemail_detection [Vapi::AssistantVoicemailDetection] These are the settings to configure or disable voicemail detection.
    #  Alternatively, voicemail detection can be configured using the
    #  model.tools=[VoicemailTool].
    #  By default, voicemail detection is disabled.
    # @param client_messages [Array<Vapi::AssistantClientMessagesItem>] These are the messages that will be sent to your Client SDKs. Default is
    #  tool-calls,user-interrupted,voice-input,workflow.node.started,assistant.started.
    #  You can check the shape of the messages in ClientMessage schema.
    # @param server_messages [Array<Vapi::AssistantServerMessagesItem>] These are the messages that will be sent to your Server URL. Default is
    #  tination-request,handoff-destination-request,user-interrupted,assistant.started.
    #  You can check the shape of the messages in ServerMessage schema.
    # @param max_duration_seconds [Float] This is the maximum number of seconds that the call will last. When the call
    #  reaches this duration, it will be ended.
    #  @default 600 (10 minutes)
    # @param background_sound [Vapi::AssistantBackgroundSound] This is the background sound in the call. Default for phone calls is 'office'
    #  and default for web calls is 'off'.
    #  You can also provide a custom sound by providing a URL to an audio file.
    # @param model_output_in_messages_enabled [Boolean] This determines whether the model's output is used in conversation history
    #  rather than the transcription of assistant's speech.
    #  Default `false` while in beta.
    #  @default false
    # @param transport_configurations [Array<Vapi::TransportConfigurationTwilio>] These are the configurations to be passed to the transport providers of
    #  assistant's calls, like Twilio. You can store multiple configurations for
    #  different transport providers. For a call, only the configuration matching the
    #  call transport provider is used.
    # @param observability_plan [Vapi::LangfuseObservabilityPlan] This is the plan for observability of assistant's calls.
    #  Currently, only Langfuse is supported.
    # @param credentials [Array<Vapi::AssistantCredentialsItem>] These are dynamic credentials that will be used for the assistant calls. By
    #  default, all the credentials are available for use in the call but you can
    #  supplement an additional credentials using this. Dynamic credentials override
    #  existing credentials.
    # @param hooks [Array<Vapi::AssistantHooksItem>] This is a set of actions that will be performed on certain events.
    # @param name [String] This is the name of the assistant.
    #  This is required when you want to transfer between assistants in a call.
    # @param voicemail_message [String] This is the message that the assistant will say if the call is forwarded to
    #  voicemail.
    #  If unspecified, it will hang up.
    # @param end_call_message [String] This is the message that the assistant will say if it ends the call.
    #  If unspecified, it will hang up without saying anything.
    # @param end_call_phrases [Array<String>] This list contains phrases that, if spoken by the assistant, will trigger the
    #  call to be hung up. Case insensitive.
    # @param compliance_plan [Vapi::CompliancePlan]
    # @param metadata [Hash{String => Object}] This is for metadata you want to store on the assistant.
    # @param background_speech_denoising_plan [Vapi::BackgroundSpeechDenoisingPlan] This enables filtering of noise and background speech while the user is talking.
    #  Features:
    #  - Smart denoising using Krisp
    #  - Fourier denoising
    #  Smart denoising can be combined with or used independently of Fourier denoising.
    #  Order of precedence:
    #  - Smart denoising
    #  - Fourier denoising
    # @param analysis_plan [Vapi::AnalysisPlan] This is the plan for analysis of assistant's calls. Stored in `call.analysis`.
    # @param artifact_plan [Vapi::ArtifactPlan] This is the plan for artifacts generated during assistant's calls. Stored in
    #  `call.artifact`.
    # @param start_speaking_plan [Vapi::StartSpeakingPlan] This is the plan for when the assistant should start talking.
    #  You should configure this if you're running into these issues:
    #  - The assistant is too slow to start talking after the customer is done
    #  speaking.
    #  - The assistant is too fast to start talking after the customer is done
    #  speaking.
    #  - The assistant is so fast that it's actually interrupting the customer.
    # @param stop_speaking_plan [Vapi::StopSpeakingPlan] This is the plan for when assistant should stop talking on customer
    #  interruption.
    #  You should configure this if you're running into these issues:
    #  - The assistant is too slow to recognize customer's interruption.
    #  - The assistant is too fast to recognize customer's interruption.
    #  - The assistant is getting interrupted by phrases that are just acknowledgments.
    #  - The assistant is getting interrupted by background noises.
    #  - The assistant is not properly stopping -- it starts talking right after
    #  getting interrupted.
    # @param monitor_plan [Vapi::MonitorPlan] This is the plan for real-time monitoring of the assistant's calls.
    #  Usage:
    #  - To enable live listening of the assistant's calls, set
    #  `monitorPlan.listenEnabled` to `true`.
    #  - To enable live control of the assistant's calls, set
    #  `monitorPlan.controlEnabled` to `true`.
    # @param credential_ids [Array<String>] These are the credentials that will be used for the assistant calls. By default,
    #  all the credentials are available for use in the call but you can provide a
    #  subset using this.
    # @param server [Vapi::Server] This is where Vapi will send webhooks. You can find all webhooks available along
    #  with their shape in ServerMessage schema.
    #  The order of precedence is:
    #  1. assistant.server.url
    #  2. phoneNumber.serverUrl
    #  3. org.serverUrl
    # @param keypad_input_plan [Vapi::KeypadInputPlan]
    # @param id [String] This is the unique identifier for the assistant.
    # @param org_id [String] This is the unique identifier for the org that this assistant belongs to.
    # @param created_at [DateTime] This is the ISO 8601 date-time string of when the assistant was created.
    # @param updated_at [DateTime] This is the ISO 8601 date-time string of when the assistant was last updated.
    # @param additional_properties [OpenStruct] Additional properties unmapped to the current class definition
    # @return [Vapi::Assistant]
    def initialize(id:, org_id:, created_at:, updated_at:, transcriber: OMIT, model: OMIT, voice: OMIT, first_message: OMIT,
                   first_message_interruptions_enabled: OMIT, first_message_mode: OMIT, voicemail_detection: OMIT, client_messages: OMIT, server_messages: OMIT, max_duration_seconds: OMIT, background_sound: OMIT, model_output_in_messages_enabled: OMIT, transport_configurations: OMIT, observability_plan: OMIT, credentials: OMIT, hooks: OMIT, name: OMIT, voicemail_message: OMIT, end_call_message: OMIT, end_call_phrases: OMIT, compliance_plan: OMIT, metadata: OMIT, background_speech_denoising_plan: OMIT, analysis_plan: OMIT, artifact_plan: OMIT, start_speaking_plan: OMIT, stop_speaking_plan: OMIT, monitor_plan: OMIT, credential_ids: OMIT, server: OMIT, keypad_input_plan: OMIT, additional_properties: nil)
      @transcriber = transcriber if transcriber != OMIT
      @model = model if model != OMIT
      @voice = voice if voice != OMIT
      @first_message = first_message if first_message != OMIT
      if first_message_interruptions_enabled != OMIT
        @first_message_interruptions_enabled = first_message_interruptions_enabled
      end
      @first_message_mode = first_message_mode if first_message_mode != OMIT
      @voicemail_detection = voicemail_detection if voicemail_detection != OMIT
      @client_messages = client_messages if client_messages != OMIT
      @server_messages = server_messages if server_messages != OMIT
      @max_duration_seconds = max_duration_seconds if max_duration_seconds != OMIT
      @background_sound = background_sound if background_sound != OMIT
      @model_output_in_messages_enabled = model_output_in_messages_enabled if model_output_in_messages_enabled != OMIT
      @transport_configurations = transport_configurations if transport_configurations != OMIT
      @observability_plan = observability_plan if observability_plan != OMIT
      @credentials = credentials if credentials != OMIT
      @hooks = hooks if hooks != OMIT
      @name = name if name != OMIT
      @voicemail_message = voicemail_message if voicemail_message != OMIT
      @end_call_message = end_call_message if end_call_message != OMIT
      @end_call_phrases = end_call_phrases if end_call_phrases != OMIT
      @compliance_plan = compliance_plan if compliance_plan != OMIT
      @metadata = metadata if metadata != OMIT
      @background_speech_denoising_plan = background_speech_denoising_plan if background_speech_denoising_plan != OMIT
      @analysis_plan = analysis_plan if analysis_plan != OMIT
      @artifact_plan = artifact_plan if artifact_plan != OMIT
      @start_speaking_plan = start_speaking_plan if start_speaking_plan != OMIT
      @stop_speaking_plan = stop_speaking_plan if stop_speaking_plan != OMIT
      @monitor_plan = monitor_plan if monitor_plan != OMIT
      @credential_ids = credential_ids if credential_ids != OMIT
      @server = server if server != OMIT
      @keypad_input_plan = keypad_input_plan if keypad_input_plan != OMIT
      @id = id
      @org_id = org_id
      @created_at = created_at
      @updated_at = updated_at
      @additional_properties = additional_properties
      @_field_set = {
        "transcriber": transcriber,
        "model": model,
        "voice": voice,
        "firstMessage": first_message,
        "firstMessageInterruptionsEnabled": first_message_interruptions_enabled,
        "firstMessageMode": first_message_mode,
        "voicemailDetection": voicemail_detection,
        "clientMessages": client_messages,
        "serverMessages": server_messages,
        "maxDurationSeconds": max_duration_seconds,
        "backgroundSound": background_sound,
        "modelOutputInMessagesEnabled": model_output_in_messages_enabled,
        "transportConfigurations": transport_configurations,
        "observabilityPlan": observability_plan,
        "credentials": credentials,
        "hooks": hooks,
        "name": name,
        "voicemailMessage": voicemail_message,
        "endCallMessage": end_call_message,
        "endCallPhrases": end_call_phrases,
        "compliancePlan": compliance_plan,
        "metadata": metadata,
        "backgroundSpeechDenoisingPlan": background_speech_denoising_plan,
        "analysisPlan": analysis_plan,
        "artifactPlan": artifact_plan,
        "startSpeakingPlan": start_speaking_plan,
        "stopSpeakingPlan": stop_speaking_plan,
        "monitorPlan": monitor_plan,
        "credentialIds": credential_ids,
        "server": server,
        "keypadInputPlan": keypad_input_plan,
        "id": id,
        "orgId": org_id,
        "createdAt": created_at,
        "updatedAt": updated_at
      }.reject do |_k, v|
        v == OMIT
      end
    end

    # Deserialize a JSON object to an instance of Assistant
    #
    # @param json_object [String]
    # @return [Vapi::Assistant]
    def self.from_json(json_object:)
      struct = JSON.parse(json_object, object_class: OpenStruct)
      parsed_json = JSON.parse(json_object)
      if parsed_json["transcriber"].nil?
        transcriber = nil
      else
        transcriber = parsed_json["transcriber"].to_json
        transcriber = Vapi::AssistantTranscriber.from_json(json_object: transcriber)
      end
      if parsed_json["model"].nil?
        model = nil
      else
        model = parsed_json["model"].to_json
        model = Vapi::AssistantModel.from_json(json_object: model)
      end
      if parsed_json["voice"].nil?
        voice = nil
      else
        voice = parsed_json["voice"].to_json
        voice = Vapi::AssistantVoice.from_json(json_object: voice)
      end
      first_message = parsed_json["firstMessage"]
      first_message_interruptions_enabled = parsed_json["firstMessageInterruptionsEnabled"]
      first_message_mode = parsed_json["firstMessageMode"]
      if parsed_json["voicemailDetection"].nil?
        voicemail_detection = nil
      else
        voicemail_detection = parsed_json["voicemailDetection"].to_json
        voicemail_detection = Vapi::AssistantVoicemailDetection.from_json(json_object: voicemail_detection)
      end
      client_messages = parsed_json["clientMessages"]
      server_messages = parsed_json["serverMessages"]
      max_duration_seconds = parsed_json["maxDurationSeconds"]
      if parsed_json["backgroundSound"].nil?
        background_sound = nil
      else
        background_sound = parsed_json["backgroundSound"].to_json
        background_sound = Vapi::AssistantBackgroundSound.from_json(json_object: background_sound)
      end
      model_output_in_messages_enabled = parsed_json["modelOutputInMessagesEnabled"]
      transport_configurations = parsed_json["transportConfigurations"]&.map do |item|
        item = item.to_json
        Vapi::TransportConfigurationTwilio.from_json(json_object: item)
      end
      if parsed_json["observabilityPlan"].nil?
        observability_plan = nil
      else
        observability_plan = parsed_json["observabilityPlan"].to_json
        observability_plan = Vapi::LangfuseObservabilityPlan.from_json(json_object: observability_plan)
      end
      credentials = parsed_json["credentials"]&.map do |item|
        item = item.to_json
        Vapi::AssistantCredentialsItem.from_json(json_object: item)
      end
      hooks = parsed_json["hooks"]&.map do |item|
        item = item.to_json
        Vapi::AssistantHooksItem.from_json(json_object: item)
      end
      name = parsed_json["name"]
      voicemail_message = parsed_json["voicemailMessage"]
      end_call_message = parsed_json["endCallMessage"]
      end_call_phrases = parsed_json["endCallPhrases"]
      if parsed_json["compliancePlan"].nil?
        compliance_plan = nil
      else
        compliance_plan = parsed_json["compliancePlan"].to_json
        compliance_plan = Vapi::CompliancePlan.from_json(json_object: compliance_plan)
      end
      metadata = parsed_json["metadata"]
      if parsed_json["backgroundSpeechDenoisingPlan"].nil?
        background_speech_denoising_plan = nil
      else
        background_speech_denoising_plan = parsed_json["backgroundSpeechDenoisingPlan"].to_json
        background_speech_denoising_plan = Vapi::BackgroundSpeechDenoisingPlan.from_json(json_object: background_speech_denoising_plan)
      end
      if parsed_json["analysisPlan"].nil?
        analysis_plan = nil
      else
        analysis_plan = parsed_json["analysisPlan"].to_json
        analysis_plan = Vapi::AnalysisPlan.from_json(json_object: analysis_plan)
      end
      if parsed_json["artifactPlan"].nil?
        artifact_plan = nil
      else
        artifact_plan = parsed_json["artifactPlan"].to_json
        artifact_plan = Vapi::ArtifactPlan.from_json(json_object: artifact_plan)
      end
      if parsed_json["startSpeakingPlan"].nil?
        start_speaking_plan = nil
      else
        start_speaking_plan = parsed_json["startSpeakingPlan"].to_json
        start_speaking_plan = Vapi::StartSpeakingPlan.from_json(json_object: start_speaking_plan)
      end
      if parsed_json["stopSpeakingPlan"].nil?
        stop_speaking_plan = nil
      else
        stop_speaking_plan = parsed_json["stopSpeakingPlan"].to_json
        stop_speaking_plan = Vapi::StopSpeakingPlan.from_json(json_object: stop_speaking_plan)
      end
      if parsed_json["monitorPlan"].nil?
        monitor_plan = nil
      else
        monitor_plan = parsed_json["monitorPlan"].to_json
        monitor_plan = Vapi::MonitorPlan.from_json(json_object: monitor_plan)
      end
      credential_ids = parsed_json["credentialIds"]
      if parsed_json["server"].nil?
        server = nil
      else
        server = parsed_json["server"].to_json
        server = Vapi::Server.from_json(json_object: server)
      end
      if parsed_json["keypadInputPlan"].nil?
        keypad_input_plan = nil
      else
        keypad_input_plan = parsed_json["keypadInputPlan"].to_json
        keypad_input_plan = Vapi::KeypadInputPlan.from_json(json_object: keypad_input_plan)
      end
      id = parsed_json["id"]
      org_id = parsed_json["orgId"]
      created_at = (DateTime.parse(parsed_json["createdAt"]) unless parsed_json["createdAt"].nil?)
      updated_at = (DateTime.parse(parsed_json["updatedAt"]) unless parsed_json["updatedAt"].nil?)
      new(
        transcriber: transcriber,
        model: model,
        voice: voice,
        first_message: first_message,
        first_message_interruptions_enabled: first_message_interruptions_enabled,
        first_message_mode: first_message_mode,
        voicemail_detection: voicemail_detection,
        client_messages: client_messages,
        server_messages: server_messages,
        max_duration_seconds: max_duration_seconds,
        background_sound: background_sound,
        model_output_in_messages_enabled: model_output_in_messages_enabled,
        transport_configurations: transport_configurations,
        observability_plan: observability_plan,
        credentials: credentials,
        hooks: hooks,
        name: name,
        voicemail_message: voicemail_message,
        end_call_message: end_call_message,
        end_call_phrases: end_call_phrases,
        compliance_plan: compliance_plan,
        metadata: metadata,
        background_speech_denoising_plan: background_speech_denoising_plan,
        analysis_plan: analysis_plan,
        artifact_plan: artifact_plan,
        start_speaking_plan: start_speaking_plan,
        stop_speaking_plan: stop_speaking_plan,
        monitor_plan: monitor_plan,
        credential_ids: credential_ids,
        server: server,
        keypad_input_plan: keypad_input_plan,
        id: id,
        org_id: org_id,
        created_at: created_at,
        updated_at: updated_at,
        additional_properties: struct
      )
    end

    # Serialize an instance of Assistant to a JSON object
    #
    # @return [String]
    def to_json(*_args)
      @_field_set&.to_json
    end

    # Leveraged for Union-type generation, validate_raw attempts to parse the given
    #  hash and check each fields type against the current object's property
    #  definitions.
    #
    # @param obj [Object]
    # @return [Void]
    def self.validate_raw(obj:)
      obj.transcriber.nil? || Vapi::AssistantTranscriber.validate_raw(obj: obj.transcriber)
      obj.model.nil? || Vapi::AssistantModel.validate_raw(obj: obj.model)
      obj.voice.nil? || Vapi::AssistantVoice.validate_raw(obj: obj.voice)
      obj.first_message&.is_a?(String) != false || raise("Passed value for field obj.first_message is not the expected type, validation failed.")
      obj.first_message_interruptions_enabled&.is_a?(Boolean) != false || raise("Passed value for field obj.first_message_interruptions_enabled is not the expected type, validation failed.")
      obj.first_message_mode&.is_a?(Vapi::AssistantFirstMessageMode) != false || raise("Passed value for field obj.first_message_mode is not the expected type, validation failed.")
      obj.voicemail_detection.nil? || Vapi::AssistantVoicemailDetection.validate_raw(obj: obj.voicemail_detection)
      obj.client_messages&.is_a?(Array) != false || raise("Passed value for field obj.client_messages is not the expected type, validation failed.")
      obj.server_messages&.is_a?(Array) != false || raise("Passed value for field obj.server_messages is not the expected type, validation failed.")
      obj.max_duration_seconds&.is_a?(Float) != false || raise("Passed value for field obj.max_duration_seconds is not the expected type, validation failed.")
      obj.background_sound.nil? || Vapi::AssistantBackgroundSound.validate_raw(obj: obj.background_sound)
      obj.model_output_in_messages_enabled&.is_a?(Boolean) != false || raise("Passed value for field obj.model_output_in_messages_enabled is not the expected type, validation failed.")
      obj.transport_configurations&.is_a?(Array) != false || raise("Passed value for field obj.transport_configurations is not the expected type, validation failed.")
      obj.observability_plan.nil? || Vapi::LangfuseObservabilityPlan.validate_raw(obj: obj.observability_plan)
      obj.credentials&.is_a?(Array) != false || raise("Passed value for field obj.credentials is not the expected type, validation failed.")
      obj.hooks&.is_a?(Array) != false || raise("Passed value for field obj.hooks is not the expected type, validation failed.")
      obj.name&.is_a?(String) != false || raise("Passed value for field obj.name is not the expected type, validation failed.")
      obj.voicemail_message&.is_a?(String) != false || raise("Passed value for field obj.voicemail_message is not the expected type, validation failed.")
      obj.end_call_message&.is_a?(String) != false || raise("Passed value for field obj.end_call_message is not the expected type, validation failed.")
      obj.end_call_phrases&.is_a?(Array) != false || raise("Passed value for field obj.end_call_phrases is not the expected type, validation failed.")
      obj.compliance_plan.nil? || Vapi::CompliancePlan.validate_raw(obj: obj.compliance_plan)
      obj.metadata&.is_a?(Hash) != false || raise("Passed value for field obj.metadata is not the expected type, validation failed.")
      obj.background_speech_denoising_plan.nil? || Vapi::BackgroundSpeechDenoisingPlan.validate_raw(obj: obj.background_speech_denoising_plan)
      obj.analysis_plan.nil? || Vapi::AnalysisPlan.validate_raw(obj: obj.analysis_plan)
      obj.artifact_plan.nil? || Vapi::ArtifactPlan.validate_raw(obj: obj.artifact_plan)
      obj.start_speaking_plan.nil? || Vapi::StartSpeakingPlan.validate_raw(obj: obj.start_speaking_plan)
      obj.stop_speaking_plan.nil? || Vapi::StopSpeakingPlan.validate_raw(obj: obj.stop_speaking_plan)
      obj.monitor_plan.nil? || Vapi::MonitorPlan.validate_raw(obj: obj.monitor_plan)
      obj.credential_ids&.is_a?(Array) != false || raise("Passed value for field obj.credential_ids is not the expected type, validation failed.")
      obj.server.nil? || Vapi::Server.validate_raw(obj: obj.server)
      obj.keypad_input_plan.nil? || Vapi::KeypadInputPlan.validate_raw(obj: obj.keypad_input_plan)
      obj.id.is_a?(String) != false || raise("Passed value for field obj.id is not the expected type, validation failed.")
      obj.org_id.is_a?(String) != false || raise("Passed value for field obj.org_id is not the expected type, validation failed.")
      obj.created_at.is_a?(DateTime) != false || raise("Passed value for field obj.created_at is not the expected type, validation failed.")
      obj.updated_at.is_a?(DateTime) != false || raise("Passed value for field obj.updated_at is not the expected type, validation failed.")
    end
  end
end
